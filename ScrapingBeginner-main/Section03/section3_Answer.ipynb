{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 【第3回】BeautifulSoupの使い方①\n",
    "\n",
    "\n",
    "前回のレクチャーでは、PythonからURLにアクセスするためのライブラリ`Requests`について学習しました。\n",
    "\n",
    "`Requests`を使うことで、対象のURLからHTML自体は取得できました。\n",
    "\n",
    "でも、自分が本当に欲しいテキスト情報については、まだ取得できていない状況です。\n",
    "\n",
    "<br>\n",
    "\n",
    "最終的な目標にしている「テキストデータの取得」には、**`Requests`で取得したHTMLを、`BeautifulSoup`で解析する必要があります。**\n",
    "\n",
    "というわけで、今回は取得したHTMLを解析するために、`BeautifulSoup`の使い方を習得していきましょう。\n",
    "\n",
    "*※動画の感想を、僕のTwitterにメンションしてツイートしていただけると嬉しいです（ ;  ; ）！*\n",
    "\n",
    "Twitter : [@hayatasuuu](https://twitter.com/hayatasuuu)\n",
    "\n",
    "## BeautifulSoupとは？\n",
    "\n",
    "`BeautifulSoup`とは、HTMLを解析するためのライブラリです。\n",
    "\n",
    "[»公式ページ :Beautiful Soup Documentation — Beautiful Soup 4.9.0 documentation](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)\n",
    "\n",
    "<br>\n",
    "\n",
    "BeautifulSoupを使ってHTMLを解析することで、自分が欲しい値を取得しやすいします。\n",
    "\n",
    "例えば、以下のフルーツリストから、フルーツの名前を取得する場合を考えてみましょう。\n",
    "\n",
    "<ol>\n",
    "    <li>りんご</li>\n",
    "    <li>バナナ</li>\n",
    "    <li>もも</li>\n",
    "</ol>\n",
    "\n",
    "こういった場合に、**BeautifulSoupを使えば「`<li>`タグに入っている情報を取得する」**といった使い方ができます。\n",
    "\n",
    "*※逆に、BeautifulSoupを使わずにフルーツを取得しようとすると「正規表現を使って`<li></li>`の中身を取り出す」といった感じになって、複雑なコードを書く必要があります。*\n",
    "\n",
    "<br>\n",
    "\n",
    "いまはリストが1つしかありませんが、通常のWebページであればリストが複数出てきてもおかしくありません。\n",
    "\n",
    "<ol>\n",
    "    <li>りんご</li>\n",
    "    <li>バナナ</li>\n",
    "    <li>もも</li>\n",
    "</ol>\n",
    "\n",
    "<ol>\n",
    "    <li>なし</li>\n",
    "    <li>ぶどう</li>\n",
    "    <li>いちご</li>\n",
    "</ol>\n",
    "\n",
    "このような場合に、「ももだけ取り出したい...」と思ったら、正規表現で取得するのは非常に大変です。\n",
    "\n",
    "なので、原理原則は「**Requestsで取得したデータは、BeautifulSoupでHTMLを解析する**」とことを覚えておきましょう！"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 取得したHTMLを、BeautifulSoupで解析する\n",
    "\n",
    "スクレイピングの流れをもう一度おさらいしておくと、以下のようになっていました。\n",
    "\n",
    "1. RequestsでHTMLを取得する\n",
    "2. 取得したHTMLを解析する(BeautifulSoup)\n",
    "3. 自分が欲しい情報を取得する\n",
    "\n",
    "「①RequestsでHTMLを取得する」については前回のレクチャーで学習していますので、今回はそれ以降の②と③をやっていきたいと思います。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 必要なライブラリのインポート\n",
    "\n",
    "まずは今回使うライブラリをインポートしていきましょう。\n",
    "\n",
    "必要になるライブラリは、前回も紹介した`Requests`と`BeautifulSoup`です。\n",
    "\n",
    "<br>\n",
    "\n",
    "新しく仮想環境を作成している場合には、以下のコマンドでライブラリのインストールをしましょう。\n",
    "\n",
    "*※実行するときは、コメントアウト(`#`)を外してください！*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Anacondaを利用している方\n",
    "# ! conda install beautifulsoup4 -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "これで今回使うライブラリの準備が完了しました。\n",
    "\n",
    "まずは前回同様に、WebページからHTMLを取得していきましょう。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RequestsでHTMLを取得する\n",
    "\n",
    "前回と同じで、Pythonの公式ページからHTML情報を取得していきたいと思います。\n",
    "\n",
    "https://www.python.org/\n",
    "\n",
    "上記のURLに対して、Requestsを使ってアクセスしてみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 変数urlに、Python公式ページのURLを入れる\n",
    "url = 'https://www.python.org/'\n",
    "\n",
    "# URLにアクセスした結果を、変数rに代入する\n",
    "r = requests.get(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "これでPython公式ページへのアクセスが完了しました。\n",
    "\n",
    "レスポンス結果からHTMLを取得するには、以下のように書きましたね。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python公式ページのHTMLを表示\n",
    "print(r.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ちなみに、取得結果の型を確認してみると、これは単なる文字列になっています。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 取得したテキストの型を確認する\n",
    "print(type(r.text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BeautifulSoupでHTMLを解析するときは、文字列を対象にします。\n",
    "\n",
    "なので、`Requests`で取得してきたHTMLは、そのままBeautifulSoupに投入して大丈夫です。\n",
    "\n",
    "<br>\n",
    "\n",
    "と、あれこれ言われてもよく分からない部分があると思うので、さっさとBeautifulSoupを使ってみましょう！笑"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 取得したHTMLを解析する\n",
    "\n",
    "結論、取得してきたHTMLを解析するには、以下のように書いてあげます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BeautifulSoupでHTMLを解析する\n",
    "soup = BeautifulSoup(r.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "この1行だけで、単なる文字列だったリクエスト結果から、あとはタグを指定するだけで欲しいデータを取得できるようになります。\n",
    "\n",
    "*※変数名の`soup`ですが、慣習的にBeautifulSoupの解析結果を、こう名付けることが多いです。*\n",
    "\n",
    "<br>\n",
    "\n",
    "あとは、ここから自分が欲しいデータを取得するだけです！"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 自分が欲しい情報を取得する\n",
    "\n",
    "今回はPythonの公式ページから、`<h2>`タグに書かれている内容を取得したいと思います。\n",
    "\n",
    "[Python公式ページ](https://www.python.org/)をデベロッパーツール(検証)で見るとわかりますが、`<h2>`タグは色々なところで使われています。\n",
    "\n",
    "<br>\n",
    "\n",
    "これらの`<h2>`タグから、まずは最初の1つである`Get Started`だけ取得していきましょう。\n",
    "\n",
    "`soup`から`<h2>`の情報を抽出するには、以下のように記述します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# soupから最初のh2を取得する\n",
    "soup.h2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上記のように書いてあげると、最初にヒットする`<h2>`をタグごと取得できます。\n",
    "\n",
    "また、`soup.h2`以外にも、以下のような書き方が可能です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# soupから最初のh2を取得する(soup.h2以外で)\n",
    "soup.find('h2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "どちらの方法でも、`<h2>`タグの情報を取得できているかと思います。\n",
    "\n",
    "ここから中身のテキストを取得するのも非常にカンタンで、以下のように書いてあげるだけです。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# soup.h2の方法でテキストを取得する\n",
    "print(soup.h2.text)\n",
    "\n",
    "# soup.find('h2')の方法でテキストを取得する\n",
    "print(soup.find('h2').text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "どちらの方法でも、同じように「Get Started」を取得できました。\n",
    "\n",
    "また、テキストを取得する方法も2つあり、以下のように書くこともできます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# soup.h2の方法でテキストを取得する\n",
    "print(soup.h2.get_text())\n",
    "\n",
    "# soup.find('h2')の方法でテキストを取得する\n",
    "print(soup.find('h2').get_text())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ここまで聞くと、なんだかややこしくなってきますよね。\n",
    "\n",
    "なので「自分はこの方法で取得するんだ！」という方法を決めておくのがオススメです。\n",
    "\n",
    "<br>\n",
    "\n",
    "参考程度に、僕がいつも使っている方法は、以下のようになっています。\n",
    "\n",
    "- タグを取得するとき : `soup.find()`\n",
    "- テキストを取得するとき : `soup.find().text`\n",
    "\n",
    "タグの取得の関しては、このあと紹介しますが複数の`<h2>`タグを取得するとき`find_all()`を使います。\n",
    "\n",
    "`find()`と`find_all()`のセットで使ってあげると分かりやすいので、1つのタグを取得するときは`find_all()`がオススメです。\n",
    "\n",
    "<br>\n",
    "\n",
    "また、`get_text()`ではなく`text`を使っているのは、1つに「簡潔に書ける」という意味があります。\n",
    "\n",
    "他には、Pythonの辞書からvalueを取得するとき、該当しないKeyで`get()`を使うとNoneが返ってくるのに対し、`get_text()`ではエラーを返します。\n",
    "\n",
    "自分の中で`get()`はNoneを返すという頭になっているので、BeautifulSoupでテキストを取得するときは`.text`を使うようにしています。\n",
    "\n",
    "<br>\n",
    "\n",
    "とは言っても、人によって好みがあると思うので、お好きな方法で要素から情報を取得してみてください(｀・ω・´)！\n",
    "\n",
    "この講義シリーズでは、自分がいつも使っている以下の方法で進めさせてくださいm(_ _)m\n",
    "\n",
    "- タグを取得するとき : `soup.find()`\n",
    "- テキストを取得するとき : `soup.find().text`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 演習\n",
    "\n",
    "[テックダイアリー(僕のブログ)](https://tech-diary.net)の以下の記事で、ブログタイトルを「文字列で」取得してみましょう。\n",
    "\n",
    "https://tech-diary.net/python-scraping-books/\n",
    "\n",
    "<br>\n",
    "\n",
    "*※できれば1回のリクエストで情報取得していただけると嬉しいです（ ;  ; ）*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://tech-diary.net/python-scraping-books/'\n",
    "\n",
    "r = requests.get(url)\n",
    "\n",
    "soup = BeautifulSoup(r.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.find('h1').text"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
